---
title: "Motion Analysis"
author: "Mitch Fawcett"
date: "November 7, 2015"
output: html_document
---
## Executive Summary
Taking a series of motion activity observations that were previously assigned one of five classifications, I fit a model that lets me predict the class of a new observation.  The model uses the Random Forests algorithm with data preprocessed using Principal Component Analysis.  The result was a model with an expected accuracy of approximately 97%.  The model successfully predicted 19 of 20 test cases.

## Approach
Because this is a classification problem I decided to use Random Forests to create the model.  Random Forests is one example of “ensemble learning” — methods that generate many classifiers and aggregate their results. Random Forests is simple to implement and is a very accurate classifier. Disadvantages are that it can be slow to process and can be prone to over-fitting.  Other downsides are it does not handle missing data very well and can be complicated to understand.

## Load source data and libraries
The data being analyzed is from a study of weight lifting exersise (source: http://groupware.les.inf.puc-rio.br/har).  The first step is to load their data into data set called "src".  This will later be divided into a training and a testing data set.
```{r, loaddata, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
setwd("/Users/mitchellfawcett/Documents/RProjects/PracticalMachineLearning")

library(kernlab)
library(caret)
library(doMC)  ## to enable multi code parallel processing
registerDoMC(cores = 2)  ## MacBook Air has two cores.  Parallel processing will happen automatically]
## library(ggplot2) will be loaded by caret

## Load source data file already in the working directory
src <- read.csv("pml-training.csv", 
                header = TRUE, 
                na.strings = "NA", 
                strip.white = TRUE)
```

## Preprocess source data
A cursory examination of the data using the summary() function shows that many variables in the source data consists of NA values or are blank.  There are also non-numeric values such as "#DIV/0!" strings throughout the data.

All blanks and division by zero values in the source data were changed to 'NA'.  Then I eliminated any column where more than 95% of the values are NA or where the values were time stamps. I also eliminated column X (column 1) that contained row numbers.  Failure to eliminate it would be problematic because it would correlate perfectly with similar values in the assignment testing set.
```{r, preprocessdata, echo=TRUE}
src[ src == "#DIV/0!" ] = NA
src[ src == "" ] = NA
src <- src[, colSums(is.na(src)) < nrow(src) * 0.05] ## (ie only keep columns that are less than 5% NA)
src <- src[, -c(1, 3:7)]
```

## Split data into training and testing data sets
I allocated 75% of the source data for training purposes and 25% for testing.
```{r, splitdata}
set.seed(32323)

inTrain <- createDataPartition(y = src$classe, 
                               p = 0.75, 
                               list = FALSE)
training <- src[inTrain,]
testing <- src[-inTrain,]
dim(training) ; dim(testing)
```

## Explore data
The following plots shows what a sample of the data "looks" like. The data consists of many motion sensor measurements being made each second.  In the graphs the x axis is "time".  The y axis is the measurement at each point in time for the "gyros_belt_x" parameter.  It is one of approximately 50 variables that will be used as predictors. Each graph contains the data for one of the 5 classes (A, B, C, D, E) for a single test person. Class A contains data collected when a barbell lift was performed correctly. The other four Classes contain data collected when the barbell was lifted incorrectly.  The goal of the model is to predict  the class (A - E) of a new set of measurements.
```{r, exploredata, echo =FALSE}
par(mfrow=c(2,3))

plot(src[training$user_name == 'adelmo' & training$classe == 'A', "gyros_belt_x"], type = 'l', ylab = "gyros_belt_x", xlab = "time" )
title(main = "Class A")
plot(src[training$user_name == 'adelmo' & training$classe == 'B', "gyros_belt_x"], type = 'l', ylab = "gyros_belt_x", xlab = "time" )
title(main = "Class B")
plot(src[training$user_name == 'adelmo' & training$classe == 'C', "gyros_belt_x"], type = 'l', ylab = "gyros_belt_x", xlab = "time" )
title(main = "Class C")
plot(src[training$user_name == 'adelmo' & training$classe == 'D', "gyros_belt_x"], type = 'l', ylab = "gyros_belt_x", xlab = "time" )
title(main = "Class D")
plot(src[training$user_name == 'adelmo' & training$classe == 'E', "gyros_belt_x"], type = 'l', ylab = "gyros_belt_x", xlab = "time" )
title(main = "Class E")
```

## Is there a high degree of correlation between variables?
Because Random Forest (like other ensemble learning methods) can take a long time to process, I decided to preprocess with Principal Component Analysis to consolidate multiple variables into a smaller number of uncorrelated principal components.  

PCA is a good strategy to use when many predictor variables are highly correlated with one another.  The following lists the pairs of predictor variables that have a correlation of 0.80 or greater.
```{r, covariantVariables, warning=FALSE}
## Matrix of corrleations between all pairs of predictor variables
M <- abs(cor(training[,2:53]))
## Goal is a list of all pairs that have a correlation > 0.80. Above and below matrix diagonal
## are mirrors. Only need one triangle so change upper triangle values to zero. Diagonal is also
## changed to zero since each variable is perfectly correlated with itself and will have a correlation of 1.
M[upper.tri(M, diag = TRUE)] <- 0
## Pairs of rows and columns that have correlation > .8
M1 <- data.frame(which(M > 0.8,arr.ind=T))
## Get the corresponding variable names of the pairs of row and column numbers
data.frame(var1 = rownames(M)[M1[,1]], var2 = colnames(M)[M1[,2]])
```

## Fit model
Since this is a classification problem, Random Forests is an appropriate choice of modeling algorithm. Repeatedcv with 10 k folds and 3 repetitions is used to provide the resampling for cross validation. The "train" function of the caret package will pick the model with the best fit by using the results of the k fold cross validation.  Preproccessing is done using Prinicipal Component Analysis ("pca").
```{r, fitmodel, cache=TRUE}

ctrl <- trainControl(method="repeatedcv", number=10, repeats=3)
modelfit <- train(classe ~ ., method = "rf", trControl = ctrl, preProcess = "pca", data = training)
# display model results
modelfit

```


## Make predictions using held back testing data
Exclude column 54 of the testing data in the prediction since it contains the known classifications A through E for each observation, which is what I am trying to predict.
```{r, predict, warning=FALSE, message=FALSE}
predictions <- predict(modelfit, newdata = testing[, -(54)])

```


## Confusion matrix for predictions made on held back testing data
The following is an analysis of the ability of the model to correctly classify observations from the held back test data.  I passed the "predictions" object created above to the confusionMatrix function and told it to calculate the results of predicting the "classe" variable on the held back testing data.
```{r, confusionMatix, warning=FALSE}
confusionMatrix(predictions, testing$classe)

```
The results indicate an estimated 97.94% overall accuracy will be possible with the model. This equates to an estimated Out-Of-Sample error rate of 2.06%.  The explanation of this is that, using the model with previously unseen data, we will expect about a 2% error rate.  In the source data, the distribution of classe values was reasonably balanced (A:5580, B:3797, C:3422, D:3216, E:3607). Overall accuracy can be used as an evaluation of classification model performance when when classes are balanced (source: https://www.r-project.org/nosvn/conferences/useR-2013/Tutorials/kuhn/user_caret_2up.pdf).  

## Load assignment testing data
```{r, loadAssignmentTestingData, warning=FALSE}
## Load source data file for the predictions to be submitted
assignmentTesting <- read.csv("pml-testing.csv", 
                header = TRUE, 
                na.strings = "NA", 
                strip.white = TRUE)

```

## Preprocess the assignment's testing data
Perform the same processing used in the training data for handling missing data, time stamps and non-predictive values.
```{r, preprocessAssignmentTestingData, warning=FALSE}
assignmentTesting[ assignmentTesting == "#DIV/0!" ] = NA
assignmentTesting[ assignmentTesting == "" ] = NA
assignmentTesting <- assignmentTesting[, colSums(is.na(assignmentTesting)) < nrow(assignmentTesting) * 0.05] ## (ie only keep columns that are less than 5% NA)
assignmentTesting <- assignmentTesting[, -c(1, 3:7)]
dim(assignmentTesting)
```

## Make predictions using assignment testing data
Make predictions for the 20 test cases provided by the assignment and put the results into a character array.
```{r, predictAssignmentTestingData, warning=FALSE}

## Make predictions using assignment testing data set and load answers into a character vector
answer <- as.character(predict(modelfit, newdata = assignmentTesting[, -(54) ])) ## hold back column 54 which is just the problem_id for each row

print(answer)
```
Above are the classifications that were predicted for the 20 test cases that were submitted for grading.  All were correct except for the third case.  The model predicted "A" but this was determined to be incorrect. This error rate is roughly in line with what was expected as the Out-Of-Sample error rate estimated in the confusion matrix of the held back testing data.

## Create files needed for submissions
This code was provided by the assignment.
```{r, createSubmissionFiles}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(answer)

```
